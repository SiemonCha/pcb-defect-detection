# Operational Workflows

This guide documents end-to-end flows for common use cases. All commands assume the
project root with the editable install already performed.

## Baseline research loop

1. **Dataset sanity checks**
   ```bash
   pcb-dd verify-setup
   pcb-dd dataset-analysis
   ```
2. **Baseline training**
   ```bash
   pcb-dd train-baseline --epochs 40 --patience 10
   ```
3. **Evaluation**
   ```bash
   pcb-dd evaluate
   pcb-dd confusion --split test
   ```
4. **Analysis bundle**
   ```bash
   pcb-dd quick-analysis --interpretability-samples 12
   ```
5. **Decision**: adjust data or hyperparameters based on reports, then repeat.

## Production model promotion

1. Train baseline (above) until metrics exceed go/no-go threshold.
2. Fine-tune the production configuration:
   ```bash
   pcb-dd train-production --epochs 120 --batch 16 --patience 20
   ```
3. Export deployable artifacts:
   ```bash
   pcb-dd export-onnx --model runs/train/production_yolov8s*/weights/best.pt
   pcb-dd quantize --model runs/train/production_yolov8s*/weights/best.pt
   ```
4. Confidence validation:
   ```bash
   pcb-dd robustness --samples 25
   pcb-dd monitor --duration 180
   ```
5. Summarise findings in `outputs/reports/` and file the release candidate.

## Hyperparameter sweep

1. Prepare working directory:
   ```bash
   mkdir -p runs/optuna
   ```
2. Launch optimisation:
   ```bash
   python -m training.hyperparameter --trials 40 --epochs 35
   ```
3. Inspect results in `runs/optuna/` and use the best settings with
   `pcb-dd train-production`.

## Cross-validation assessment

1. Ensure dataset contains both `train/` and `valid/` splits.
2. Run cross-validation:
   ```bash
   python -m training.cross_validation --folds 5 --epochs 60
   ```
3. Review aggregate statistics printed to the console and stored in
   `runs/cv/summary.json` (auto-generated by the script).

## API deployment checklist

1. Export ONNX/INT8 artifacts (see production flow).
2. Start the API:
   ```bash
   pcb-dd api --model outputs/models/best_int8.onnx --host 0.0.0.0 --port 8000
   ```
3. Run smoke tests:
   ```bash
   pytest tests/test_api.py -k "not skip" --maxfail=1 --disable-warnings
   ```
4. Run the monitoring tool against the live server:
   ```bash
   pcb-dd monitor --api-url http://localhost:8000 --duration 300
   ```
5. Capture artefacts (logs, benchmark results) in a release bundle.

## Notebook-driven experimentation

1. Install optional analytics dependencies:
   ```bash
   pip install -e ".[analytics]"
   ```
2. Start Jupyter Lab with the project environment activated.
3. Use `from pcb_defect_detection.config import load_config` to access configuration
   within notebooks.

Refer back to the [Troubleshooting](troubleshooting.md) guide if any step fails.

## Containerised workflows

1. Build images:
   ```bash
   cd docker
   docker compose build
   ```
2. Launch the API container:
   ```bash
   docker compose up api
   ```
3. Run an ad-hoc training job:
   ```bash
   docker compose run --rm trainer pcb-dd train-baseline --epochs 20
   ```

Mounts expose `runs/`, `logs/`, `outputs/`, and `datasets/` to the host. Adjust the
command to execute other CLI tasks (evaluation, quantisation, etc.).
